{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "This notebook preprocesses MovieLens 32M and TMDB datasets to create three model-specific datasets:\n",
    "1. **Collaborative Filtering**: User-item interaction matrix\n",
    "2. **Content-Based Filtering**: Movie metadata with embeddings\n",
    "3. **Two-Tower Model**: Combined ratings and movie features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to kaggle\n",
    "import kagglehub as kh\n",
    "import os\n",
    "\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading datasets\n",
    "movie_path = kh.dataset_download(\"asaniczka/tmdb-movies-dataset-2023-930k-movies\")\n",
    "ratings_path = kh.dataset_download(\"justsahil/movielens-32m\")\n",
    "\n",
    "print(f\"Movies path: {movie_path}\")\n",
    "print(f\"Ratings path: {ratings_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data with Polars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "movie_df = pl.read_csv(movie_path + \"/TMDB_movie_dataset_v11.csv\")\n",
    "ratings_df = pl.read_csv(ratings_path + \"/ml-32m/ratings.csv\")\n",
    "links_df = pl.read_csv(ratings_path + \"/ml-32m/links.csv\")\n",
    "\n",
    "print(f\"Movies: {movie_df.shape}\")\n",
    "print(f\"Ratings: {ratings_df.shape}\")\n",
    "print(f\"Links: {links_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Early Column Filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = movie_df.select(\n",
    "    [\n",
    "        \"id\",\n",
    "        \"title\",\n",
    "        \"overview\",\n",
    "        \"tagline\",\n",
    "        \"genres\",\n",
    "        \"keywords\",\n",
    "        \"vote_average\",\n",
    "        \"vote_count\",\n",
    "        \"runtime\",\n",
    "        \"release_date\",\n",
    "        \"original_language\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Columns: {movie_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handle Invalid Ratings\n",
    "\n",
    "Ratings < 0.5 indicate no rating (not an actual 0.5 rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_count = ratings_df.filter(pl.col(\"rating\") < 0.5).shape[0]\n",
    "print(f\"Invalid ratings: {invalid_count}\")\n",
    "\n",
    "ratings_df = ratings_df.with_columns(\n",
    "    pl.when(pl.col(\"rating\") < 0.5)\n",
    "    .then(None)\n",
    "    .otherwise(pl.col(\"rating\"))\n",
    "    .alias(\"rating\")\n",
    ")\n",
    "\n",
    "print(f\"Set to null: {invalid_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ratings with links\n",
    "merged_df = ratings_df.join(links_df, on=\"movieId\", how=\"inner\")\n",
    "\n",
    "# Merge with movie metadata\n",
    "merged_df = merged_df.join(movie_df, left_on=\"tmdbId\", right_on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.with_columns(\n",
    "    [\n",
    "        pl.col(\"overview\").fill_null(\"\"),\n",
    "        pl.col(\"tagline\").fill_null(\"\"),\n",
    "        pl.col(\"genres\").fill_null(\"\"),\n",
    "        pl.col(\"keywords\").fill_null(\"\"),\n",
    "        pl.col(\"runtime\").fill_null(pl.col(\"runtime\").median()),\n",
    "        pl.col(\"vote_average\").fill_null(0),\n",
    "        pl.col(\"vote_count\").fill_null(0),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Drop rows with missing id or title\n",
    "initial_rows = merged_df.shape[0]\n",
    "merged_df = merged_df.drop_nulls(subset=[\"movieId\", \"title\"])\n",
    "print(f\"Dropped {initial_rows - merged_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Dataset 1: Collaborative Filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out null ratings and select only necessary columns\n",
    "ratings_cf = merged_df.filter(pl.col(\"rating\").is_not_null())\n",
    "ratings_cf = ratings_cf.select([\"userId\", \"movieId\", \"tmdbId\", \"rating\", \"timestamp\"])\n",
    "\n",
    "# Calculate sparsity\n",
    "n_users = ratings_cf[\"userId\"].n_unique()\n",
    "n_movies = ratings_cf[\"tmdbId\"].n_unique()\n",
    "sparsity = 1 - (ratings_cf.shape[0] / (n_users * n_movies))\n",
    "print(f\"Matrix sparsity: {sparsity:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_cf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Dataset 2: Content-Based Filtering\n",
    "\n",
    "Process movie metadata and generate embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique movies\n",
    "movies_cb = movie_df.unique(subset=[\"id\"])\n",
    "\n",
    "print(f\"Unique movies: {movies_cb.shape[0]:,}\")\n",
    "print(f\"Columns: {movies_cb.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the feature columns\n",
    "\n",
    "We want to merge the feature columns into one single column so that we have a one string to have embeddings from.\n",
    "I've decided to merge `title`, `overview`, `tagline`, `genres`, `keywords` columns, since they will be the most important for content based filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to pandas\n",
    "movies_cb_pd = movies_cb.to_pandas()\n",
    "movies_cb_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# remove emojis\n",
    "movies_cb_pd[\"overview\"] = movies_cb_pd[\"overview\"].apply(\n",
    "    lambda x: re.sub(r\"[^\\x00-\\x7F]+\", \"\", x)\n",
    ")\n",
    "movies_cb_pd[\"tagline\"] = movies_cb_pd[\"tagline\"].apply(\n",
    "    lambda x: re.sub(r\"[^\\x00-\\x7F]+\", \"\", x)\n",
    ")\n",
    "\n",
    "movies_cb_pd[\"merged_text\"] = (\n",
    "    movies_cb_pd[\"title\"].fillna(\"\")\n",
    "    + \" \"\n",
    "    + movies_cb_pd[\"overview\"].fillna(\"\")\n",
    "    + \" \"\n",
    "    + movies_cb_pd[\"tagline\"].fillna(\"\")\n",
    "    + \" \"\n",
    "    + movies_cb_pd[\"genres\"].fillna(\"\")\n",
    "    + \" \"\n",
    "    + movies_cb_pd[\"keywords\"].fillna(\"\")\n",
    ").str.strip()\n",
    "\n",
    "# Clean up extra spaces\n",
    "movies_cb_pd[\"merged_text\"] = movies_cb_pd[\"merged_text\"].str.replace(\n",
    "    r\"\\s+\", \" \", regex=True\n",
    ")\n",
    "\n",
    "print(f\"Average text length: {movies_cb_pd['merged_text'].str.len().mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cb_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate TF-IDF Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse as sp\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000, stop_words=\"english\", ngram_range=(1, 2), min_df=2, max_df=0.8\n",
    ")\n",
    "\n",
    "tfidf_embeddings = tfidf_vectorizer.fit_transform(movies_cb_pd[\"merged_text\"])\n",
    "\n",
    "print(f\"TF-IDF shape: {tfidf_embeddings.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(\n",
    "    f\"Sparsity: {tfidf_embeddings.nnz / (tfidf_embeddings.shape[0] * tfidf_embeddings.shape[1]):.4%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate BERT Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if MPS is available for MacOS\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load model (huggingface)\n",
    "bert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Encode\n",
    "bert_embeddings = bert_model.encode(\n",
    "    movies_cb_pd[\"merged_text\"].tolist(),\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    ")\n",
    "\n",
    "print(f\"BERT embeddings shape: {bert_embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {bert_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to Polars with essential columns\n",
    "\n",
    "movies_cb_final = movies_cb_pd[\n",
    "    [\n",
    "        \"id\",\n",
    "        \"title\",\n",
    "        \"overview\",\n",
    "        \"tagline\",\n",
    "        \"genres\",\n",
    "        \"vote_average\",\n",
    "        \"vote_count\",\n",
    "        \"runtime\",\n",
    "        \"release_date\",\n",
    "        \"original_language\",\n",
    "        \"merged_text\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Convert back to Polars for efficient storage\n",
    "movies_cb = pl.from_pandas(movies_cb_final)\n",
    "\n",
    "print(f\"Content-based dataset ready: {movies_cb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Dataset 3: Two-Tower Model Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select movie features for item tower\n",
    "movie_features = movies_cb.select(\n",
    "    [\"id\", \"merged_text\", \"genres\", \"vote_average\", \"vote_count\", \"runtime\"]\n",
    ")\n",
    "\n",
    "# Join with ratings\n",
    "two_tower_df = ratings_cf.join(\n",
    "    movie_features, left_on=\"tmdbId\", right_on=\"id\", how=\"inner\"\n",
    ")\n",
    "\n",
    "print(f\"Two-tower training samples: {two_tower_df.shape[0]:,}\")\n",
    "print(f\"Unique users: {two_tower_df['userId'].n_unique():,}\")\n",
    "print(f\"Unique movies: {two_tower_df['tmdbId'].n_unique():,}\")\n",
    "print(f\"\\nColumns: {two_tower_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_tower_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Validation & Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Convert to pandas for plotting\n",
    "ratings_cf_pd = ratings_cf.to_pandas()\n",
    "\n",
    "# Rating distribution\n",
    "axes[0].hist(ratings_cf_pd[\"rating\"], bins=10, edgecolor=\"black\", color=\"skyblue\")\n",
    "axes[0].set_title(\"Rating Distribution\")\n",
    "axes[0].set_xlabel(\"Rating\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Ratings per user\n",
    "user_counts = ratings_cf_pd.groupby(\"userId\").size()\n",
    "axes[1].hist(user_counts, bins=50, edgecolor=\"black\", color=\"lightcoral\")\n",
    "axes[1].set_title(\"Ratings per User\")\n",
    "axes[1].set_xlabel(\"Number of Ratings\")\n",
    "axes[1].set_ylabel(\"Number of Users\")\n",
    "axes[1].set_yscale(\"log\")\n",
    "\n",
    "# Ratings per movie\n",
    "movie_counts = ratings_cf_pd.groupby(\"tmdbId\").size()\n",
    "axes[2].hist(movie_counts, bins=50, edgecolor=\"black\", color=\"lightgreen\")\n",
    "axes[2].set_title(\"Ratings per Movie\")\n",
    "axes[2].set_xlabel(\"Number of Ratings\")\n",
    "axes[2].set_ylabel(\"Number of Movies\")\n",
    "axes[2].set_yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"\\nRatings per user - Min: {user_counts.min()}, Max: {user_counts.max()}, Median: {user_counts.median():.0f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Ratings per movie - Min: {movie_counts.min()}, Max: {movie_counts.max()}, Median: {movie_counts.median():.0f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Processed Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "output_dir = \"data/processed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Collaborative Filtering\n",
    "ratings_cf.write_parquet(f\"{output_dir}/ratings_cf.parquet\")\n",
    "\n",
    "# 2. Content-Based Filtering\n",
    "movies_cb.write_parquet(f\"{output_dir}/movies_cb.parquet\")\n",
    "np.save(f\"{output_dir}/bert_embeddings_cb.npy\", bert_embeddings)\n",
    "sp.save_npz(f\"{output_dir}/tfidf_embeddings_cb.npz\", tfidf_embeddings)\n",
    "joblib.dump(tfidf_vectorizer, f\"{output_dir}/tfidf_vectorizer.pkl\")\n",
    "\n",
    "# 3. Two-Tower Model\n",
    "two_tower_df.write_parquet(f\"{output_dir}/two_tower_train.parquet\")\n",
    "\n",
    "# 4. Metadata\n",
    "metadata = {\n",
    "    \"collaborative_filtering\": {\n",
    "        \"num_ratings\": int(ratings_cf.shape[0]),\n",
    "        \"num_users\": int(ratings_cf[\"userId\"].n_unique()),\n",
    "        \"num_movies\": int(ratings_cf[\"tmdbId\"].n_unique()),\n",
    "        \"avg_rating\": float(ratings_cf[\"rating\"].mean()),\n",
    "        \"sparsity\": float(sparsity),\n",
    "    },\n",
    "    \"content_based\": {\n",
    "        \"num_movies\": int(movies_cb.shape[0]),\n",
    "        \"tfidf_dim\": tfidf_embeddings.shape[1],\n",
    "        \"bert_dim\": bert_embeddings.shape[1],\n",
    "        \"bert_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    },\n",
    "    \"two_tower\": {\n",
    "        \"num_samples\": int(two_tower_df.shape[0]),\n",
    "        \"num_users\": int(two_tower_df[\"userId\"].n_unique()),\n",
    "        \"num_movies\": int(two_tower_df[\"tmdbId\"].n_unique()),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "with open(f\"metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
